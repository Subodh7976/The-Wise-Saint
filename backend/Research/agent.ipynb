{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END \n",
    "from typing import TypedDict, List, Any\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver \n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "MAX_REVIEW_COUNT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    query: str \n",
    "    definition: str \n",
    "    decomposed_steps: List[str]\n",
    "    solved_steps: List[str]\n",
    "    step_history: List[Any]\n",
    "    completed_step: int\n",
    "    review_count: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Steps(BaseModel):\n",
    "    steps: List[str]\n",
    "\n",
    "# class StepPlan(BaseModel):\n",
    "#     response: str = Field(\n",
    "#         \"Either contains solution of the step if calculator is not needed or\" \\\n",
    "#             \" includes the query for the calculator\"\n",
    "#     )\n",
    "#     use_calculator: bool = Field(\n",
    "#         \"defines whether to use calculator or the solution is already finished\"\n",
    "#     )\n",
    "\n",
    "class StepPlan(BaseModel):\n",
    "    response: str \n",
    "    use_calculator: bool\n",
    "    review_steps: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_definer_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PROBLEM_DEFINER_PROMPT),\n",
    "        HumanMessage(content=state['query'])\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"definition\": response.content,\n",
    "        \"review_count\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_decomposer_node(state: AgentState):\n",
    "    parser = PydanticOutputParser(pydantic_object=Steps)\n",
    "    \n",
    "    chain = llm | parser\n",
    "    system_message = f\"{PROBLEM_DECOMPOSER_PROMPT}\\n\\nFormat Instructions: {parser.get_format_instructions()}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=state['definition'])\n",
    "    ]\n",
    "    \n",
    "    steps = chain.invoke(messages)\n",
    "    return {\n",
    "        \"decomposed_steps\": steps.steps, \n",
    "        \"completed_steps\": 0, \n",
    "        \"step_history\": [], \n",
    "        \"solved_steps\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_iterator(state: AgentState):\n",
    "    print(\"Steps completed - \", state['completed_step'])\n",
    "    if (state['completed_step'] or 0) >= len(state['decomposed_steps']):\n",
    "        return \"refine_steps\"\n",
    "    return \"generate_step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "def step_generator(state: AgentState):\n",
    "    solved_steps = state['solved_steps'] or []\n",
    "    step_count = state['completed_step'] or 0\n",
    "    messages = state['step_history'] or []\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=StepPlan)\n",
    "    chain = llm | parser\n",
    "    \n",
    "    if len(messages) == 0:\n",
    "        system_message = STEP_GENERATOR_PROMPT.format(format_instructions=parser.get_format_instructions())\n",
    "        content = \"\\n\\n\".join(solved_steps)\n",
    "        human_content = f\"Original problem definition: {state['query']}. This is the solution so far: {content}\\n\\n and this is the description for next step: {state['decomposed_steps'][step_count]}\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=human_content)\n",
    "        ]\n",
    "    \n",
    "    retry = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = chain.invoke(messages)\n",
    "            break\n",
    "        except OutputParserException as e:\n",
    "            retry += 1\n",
    "            if retry > 2:\n",
    "                raise e\n",
    "            continue\n",
    "    messages.append(AIMessage(content=response.json()))\n",
    "    \n",
    "    return {\n",
    "        \"step_history\": messages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_tool_decider(state: AgentState):\n",
    "    parser = PydanticOutputParser(pydantic_object=StepPlan)\n",
    "    last_response = state['step_history'][-1]\n",
    "    last_response = parser.parse(last_response.content)\n",
    "    \n",
    "    if last_response.use_calculator:\n",
    "        return \"calculator\"\n",
    "    if last_response.review_steps:\n",
    "        review_count = state['review_count'] or 0\n",
    "        if review_count < MAX_REVIEW_COUNT:\n",
    "            review_count += 1\n",
    "            return \"reset_steps\"\n",
    "    return \"update_steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_steps(state: AgentState):\n",
    "    parser = PydanticOutputParser(pydantic_object=StepPlan)\n",
    "    last_response = state['step_history'][-1]\n",
    "    last_response = parser.parse(last_response.content)\n",
    "    \n",
    "    solved_steps = state['solved_steps'] or []\n",
    "    content = \"\\n\\n\".join(solved_steps)\n",
    "    content += last_response.response\n",
    "    \n",
    "    content = \"Problem Definition:\\n\" + state['definition'] + \"\\n\\nSTEPS AND FEEDBACK:\\n\" + content\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=STEPS_REVIEW_PROMPT),\n",
    "        HumanMessage(content=content)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"definition\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_steps(state: AgentState):\n",
    "    parser = PydanticOutputParser(pydantic_object=StepPlan)\n",
    "    last_response = state['step_history'][-1]\n",
    "    last_response = parser.parse(last_response.content)\n",
    "    \n",
    "    step_count = state['completed_step'] or 0\n",
    "    step_count += 1\n",
    "    solved_steps = state['solved_steps'] or []\n",
    "    solved_steps.append(last_response.response)\n",
    "    \n",
    "    return {\n",
    "        \"completed_step\": step_count, \n",
    "        \"solved_steps\": solved_steps, \n",
    "        \"step_history\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "\n",
    "def calculator(state: AgentState):\n",
    "    parser = PydanticOutputParser(pydantic_object=StepPlan)\n",
    "    last_response = state['step_history'][-1]\n",
    "    last_response = parser.parse(last_response.content)\n",
    "    \n",
    "    wolfram = WolframAlphaAPIWrapper()\n",
    "    response = wolfram.run(last_response.response)\n",
    "    human_content = f\"With calculator response: {response}. Generate the whole solution.\"\n",
    "    messages = state['step_history']\n",
    "    messages.append(HumanMessage(\n",
    "        content=human_content\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        \"step_history\": messages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refiner(state: AgentState):\n",
    "    solved_steps = state['solved_steps'] or []\n",
    "    content = \"\\n\\n\".join(solved_steps)\n",
    "    \n",
    "    query = state['query']\n",
    "    human_content = f\"Problem Definition: {query}\\n\\nSolved Steps: {content}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=REFINER_PROMPT),\n",
    "        HumanMessage(content=human_content)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    solved_steps.append(response.content)\n",
    "    return {\n",
    "        \"solved_steps\": solved_steps\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def step_generator(state: AgentState):\n",
    "#     solved_steps = state['solved_steps'] or []\n",
    "#     step_count = state.get(\"completed_steps\", 0)\n",
    "#     while step_count != len(state['decomposed_steps']):\n",
    "#         content = \"\\n\\n\".join(solved_steps)\n",
    "#         human_content = f\"Original problem definition: {state['query']}. This is the solution so far: {content}\\n\\n and this is the description for next step: {state['decomposed_steps'][step_count]}\"\n",
    "        \n",
    "#         messages = [\n",
    "#             SystemMessage(content=STEP_GENERATOR_PROMPT),\n",
    "#             HumanMessage(content=human_content)\n",
    "#         ]\n",
    "#         response = llm.invoke(messages)\n",
    "#         solved_steps.append(response.content)\n",
    "#         step_count += 1\n",
    "    \n",
    "#     return {\n",
    "#         \"solved_steps\": solved_steps, \n",
    "#         \"completed_steps\": step_count\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_node(\"definer\", problem_definer_node)\n",
    "builder.add_node(\"decomposer\", problem_decomposer_node)\n",
    "builder.add_node(\"solver\", step_generator)\n",
    "builder.add_node(\"calculator\", calculator)\n",
    "builder.add_node(\"steps_updater\", update_steps)\n",
    "builder.add_node(\"steps_reviewer\", review_steps)\n",
    "builder.add_node(\"response_refiner\", refiner)\n",
    "builder.add_node(\"gateway_node\", lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"definer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_edge(\"definer\", \"decomposer\")\n",
    "builder.add_edge(\"decomposer\", \"gateway_node\")\n",
    "builder.add_edge(\"steps_updater\", \"gateway_node\")\n",
    "builder.add_edge(\"calculator\", \"solver\")\n",
    "builder.add_edge(\"steps_reviewer\", \"decomposer\")\n",
    "builder.add_edge(\"response_refiner\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"gateway_node\", \n",
    "    step_iterator, \n",
    "    {\n",
    "        \"refine_steps\": \"response_refiner\", \n",
    "        \"generate_step\": \"solver\"\n",
    "    }\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"solver\", \n",
    "    step_tool_decider, \n",
    "    {\n",
    "        \"calculator\": \"calculator\",\n",
    "        \"update_steps\": \"steps_updater\",\n",
    "        \"reset_steps\": \"steps_reviewer\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": 2}}\n",
    "\n",
    "query = \"\"\" \n",
    "sin^2(x) + cos^2(x) = 1, prove that\n",
    "\"\"\"\n",
    "\n",
    "response = graph.invoke({\"query\": query}, thread)\n",
    "# for output in graph.stream({\"query\": query}, thread, stream_mode=\"updates\"):\n",
    "#     # stream() yields dictionaries with output keyed by node name\n",
    "#     for key, value in output.items():\n",
    "#         print(f\"Output from node '{key}':\")\n",
    "#         print(\"---\")\n",
    "#         print(value)\n",
    "#     print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\".join(response['solved_steps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['decomposed_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['definition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
